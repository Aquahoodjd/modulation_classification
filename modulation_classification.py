# -*- coding: utf-8 -*-
"""Modulation Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJNIpgPDIXGpTPI_RTyiXi7ifNo5O3Q0

# **Data Load & preprocessing**

## Connect to google drive
"""

!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# To access google Drive\
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1r0xGZe3sjlumL7IUmV-E7_OVT3tO28mY'})
download.GetContentFile('RML2016.10b.tar.bz2')

# Unzipping data folder
!apt-get install p7zip-full
!p7zip -d RML2016.10b.tar.bz2
!tar -xvf RML2016.10b.tar.bz2

import pickle
import numpy as np

dataFile = open('RML2016.10b.dat', 'rb')
#use encoding parameter as data was pickled in python2
#Data was originally structured as a dict
dataDict = pickle.load(dataFile, encoding='latin1')   
dataFile.close()

print(dataDict)

"""## Reshape & split data"""

data = []
labels = []
for d in dataDict:
  signals = dataDict[d]
  for s in signals:
    data.append(np.array(s))
    labels.append(d)

data = np.array(data)
labels = np.array(labels)
print(data.shape) 
print(labels.shape)
print(labels)

import random

indices = random.sample(range(0,1200000), 1200000)

xTrain = []
yTrain = []
iTrain = []
for i in range(0,600000):
  xTrain.append(data[indices[i]])
  yTrain.append(labels[indices[i]])
  iTrain.append(indices[i])
xTrain = np.array(xTrain)
yTrain = np.array(yTrain)
iTrain = np.array(iTrain)

xTest = []
yTest = []
iTest = []
for i in range(600000, 1200000):
  xTest.append(data[indices[i]])
  yTest.append(labels[indices[i]])
  iTest.append(indices[i])
xTest = np.array(xTrain)
yTest = np.array(yTrain)
iTest = np.array(iTest)  

print(xTrain.shape)
print(yTest.shape)
print(xTrain)

"""## Extra Features"""

import scipy

dataDifferentiated = []

scipy.integrate.simps(a)

"""## One-hot encoding of labels"""

#Sklearn’s one hot encoder doesn’t actually know how to convert categories to numbers, 
#it only knows how to convert numbers to binary. We have to use the labelencoder first.

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
le = LabelEncoder()

yTrain_SNR = yTrain[:,1]
yTest_SNR = yTest[:,1]

print(yTest[400000])
print(yTest_SNR[400000])

yTrain_le = le.fit_transform(yTrain[:, 0])
yTest_le = le.fit_transform(yTest[:, 0])
print(yTest_le[400000])

yTrain_le = yTrain_le.reshape(len(yTrain_le), 1)
yTest_le = yTest_le.reshape(len(yTest_le), 1)

enc = OneHotEncoder(categorical_features = [0])
yTrain_ohe = enc.fit_transform(yTrain_le).toarray()
yTest_ohe = enc.fit_transform(yTest_le).toarray()
print(yTest_ohe[400000])

"""# **Fully Connected Neural Network**

## Load Data (Reformated)
"""

import numpy as np


######################## 1. Load Data ########################

## xTrain / xTest -> 600000x256
#   rows: number of samples
#   cols: concatinated 2X128 features of each sample

# Reformated xTrain
# print(xTrain.shape[0])

xTrain_reforamted = []
for i in range(0,xTrain.shape[0]):
  xTrain_reforamted.append(np.concatenate((xTrain[i,0,:] , xTrain[i,1,:]), axis=0))  
xTrain_reforamted = np.array(xTrain_reforamted)
 
print("Reformated Train size")
print(xTrain_reforamted.shape)
  
  
# Reformated xTest
# print(xTest.shape[0])

xTest_reforamted = []
for i in range(0,xTest.shape[0]):
  xTest_reforamted.append(np.concatenate((xTest[i,0,:] , xTest[i,1,:]), axis=0)) 
xTest_reforamted = np.array(xTest_reforamted)
  
print("Reformated Test size")
print(xTest_reforamted.shape)
  
  
## 1 sample Test
# sample = np.concatenate((xTrain[0,0,:] , xTrain[0,1,:]), axis=0)
# print("Sample size")
# print(sample.size)

from sklearn.model_selection import train_test_split

## split 5% as validation set from xTrain and yTrain Datasets
#   using scklearn function for splitting train/test data

# xTrain_reforamted, xValid, yTrain_ohe, yValid = train_test_split(xTrain_reforamted, yTrain_ohe, test_size=0.05, shuffle= True)

print("xTrain Size")
print(xTrain_reforamted.shape)

print("yTrain Size")
print(yTrain_ohe.shape)
# print(yTrain_ohe.shape)


# print("xValid Size")
# print(xValid.shape)

# print("yValid Size")
# print(yValid.shape)

## yTrain / yTest - > 600000x11
#   rows: number of samples
#   cols: 11 classification classes (one 1 only from all 11 in each row)

print("yTest Size")
print(yTest_ohe.shape)

"""## Define, Compile, Train, Evaluate  Model  --  (Single Configuration for NN)"""

from keras.models import Model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Input
from keras.callbacks import ModelCheckpoint

######################## 2. Define Model ########################

# create model
model = Sequential()

# define Layers
model.add(Dense(512, input_dim=256, activation='relu'))

model.add(Dense(135, activation='relu'))

#soft max as O/P activation -> multiclass classification 
model.add(Dense(10, activation='softmax'))



######################## 3. Compile Model ########################

# training a network means finding the best set of weights to make predictions for this problem.
# we must specify: 
#   1. The loss function to use to evaluate a set of weights.
#   2. The optimizer used to search through different weights for the network
#   3. Any optional metrics we would like to collect and report during training.

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from keras.callbacks import EarlyStopping, ModelCheckpoint
	

######################## 4. Fit (Train) Model ########################

# checkpoint
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]


#arguments to specify:
# 1. nepochs: a fixed number of iterations through the dataset 
# 2. batch_size: the number of instances that are evaluated before a weight update in the network is performed

# Fit the model
model.fit(xTrain_reforamted, yTrain_ohe, validation_split=0.05,shuffle=True, epochs=100, batch_size=1024, callbacks=callbacks_list, verbose=0)


# es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=30)
# mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)
# model.fit(xTrain, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1000, epochs=100, verbose=1, callbacks=[es, mc])

# Model Summary
print("Model Summary")
print(model.summary())

######################## 5. Evaluate Model ########################

#evaluating our model on the taining set can help us to determine 
# how much the model overfitt/ underfits the data

print("Testing on test set")
score = model.evaluate(xTest_reforamted, yTest_ohe)
print("\n%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

"""## HyperParameter Tuning"""

##prepare constant validation set
from sklearn.model_selection import train_test_split

## split 5% as validation set from xTrain and yTrain Datasets
#   using scklearn function for splitting train/test data

xTrain_NU, xValid, yTrain_NU, yValid = train_test_split(xTrain_reforamted, yTrain_ohe, test_size=0.05, shuffle= True)

import random

## Choosing best NN paramter values:  number of hidden layers - number nerurons/layer
hidden_layers = [1, 2, 3]
neurons_per_layer = [135, 256, 225, 450, 512, 850, 1125]


### try possible combinations for hidden layers and their neurons
for layer_num in hidden_layers:
  
  print("-------------------------------------------------------------------------")
  print("Hidden Layers: ",layer_num)
  
  # create model
  model_tuned = Sequential()

  # define Layers
  x = random.choice(neurons_per_layer)
  print("Hidden Layer 1: ",x)
  model_tuned.add(Dense(x, input_dim=256, activation='relu'))
  
  for i in range (0,layer_num-1):
    x = random.choice(neurons_per_layer)
    print("Hidden Layer ",i+2, "Neurons: ",x)
    model_tuned.add(Dense(x, activation='relu'))
   
  #soft max as O/P activation -> multiclass classification 
  model_tuned.add(Dense(10, activation='softmax'))
  
  # checkpoint
  filepath="weights.best.hdf5"
  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
  callbacks_list = [checkpoint]

  # Fit the model
  model_tuned.fit(xTrain_reforamted, yTrain_ohe, validation_split=0.05, shuffle=True, epochs=100, batch_size=1024, callbacks=callbacks_list, verbose=0)

  # Model Summary
  print("Model Summary")
  print(model_tuned.summary())
  
  #Evaluate on validation set
  print("Testing on test set")
  score = model.evaluate(xValid, xValid)
  print("\n%s: %.2f%%" % (model.metrics_names[1], score[1]*100))
  
  print("-------------------------------------------------------------------------")

"""# Accuracy Plot"""

from keras.models import load_model
saved_model = load_model('weights.best.hdf5')

SNR = np.unique(labels[:,1])
SNR = list(map(int, SNR))
SNR.sort()
SNR = list(map(str, SNR))

accuracy = []

for s in SNR:
  y = []
  x = []
  M = np.where(yTest_SNR == s)
  for m in M[0]:
    y.append(yTest_ohe[m])
    x.append(xTest_reforamted[m])
    
  y = np.array(y)
  x = np.array(x)
  loss, acc = saved_model.evaluate(x, y)
  print('Accuracy for SNR = ', s, ' is ', acc)
  accuracy.append(acc)
  
print(accuracy)

import matplotlib.pyplot as plt
plt.ylim(0, 1.0)
plt.plot(SNR, accuracy)