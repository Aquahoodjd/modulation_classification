# -*- coding: utf-8 -*-
"""Assignment 3 - Modulation Recognition_M6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zGdAdPL2hSsUEXJ9JNBkb7BBbT1b68u-

##Data Load & preprocessing

###Connect to google drive
"""

!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# To access google Drive\
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1r0xGZe3sjlumL7IUmV-E7_OVT3tO28mY'})
download.GetContentFile('RML2016.10b.tar.bz2')

# Unzipping data folder
!apt-get install p7zip-full
!p7zip -d RML2016.10b.tar.bz2
!tar -xvf RML2016.10b.tar.bz2

import pickle
import numpy as np

dataFile = open('RML2016.10b.dat', 'rb')
#use encoding parameter as data was pickled in python2
#Data was originally structured as a dict
dataDict = pickle.load(dataFile, encoding='latin1')   
dataFile.close()

print(dataDict)

"""###Reshape & split data"""

data = []
labels = []
for d in dataDict:
  signals = dataDict[d]
  for s in signals:
    data.append(np.array(s))
    labels.append(d)

data = np.array(data)
labels = np.array(labels)
print(data.shape) 
print(labels.shape)
print(labels)

import random

indices = random.sample(range(0,1200000), 1200000)

xTrain = []
yTrain = []
iTrain = []
for i in range(0,600000):
  xTrain.append(data[indices[i]])
  yTrain.append(labels[indices[i]])
  iTrain.append(indices[i])
xTrain = np.array(xTrain)
yTrain = np.array(yTrain)
iTrain = np.array(iTrain)

xTest = []
yTest = []
iTest = []
for i in range(600000, 1200000):
  xTest.append(data[indices[i]])
  yTest.append(labels[indices[i]])
  iTest.append(indices[i])
xTest = np.array(xTrain)
yTest = np.array(yTrain)
iTest = np.array(iTest)  

print(xTrain.shape)
print(yTest.shape)
print(xTrain)

"""###One-hot encoding of labels"""

#Sklearn’s one hot encoder doesn’t actually know how to convert categories to numbers, 
#it only knows how to convert numbers to binary. We have to use the labelencoder first.

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
le = LabelEncoder()

yTrain_SNR = yTrain[:,1]
yTest_SNR = yTest[:,1]

print(yTest[400000])
print(yTest_SNR[400000])

yTrain_le = le.fit_transform(yTrain[:, 0])
yTest_le = le.fit_transform(yTest[:, 0])
print(yTest_le[400000])

yTrain_le = yTrain_le.reshape(len(yTrain_le), 1)
yTest_le = yTest_le.reshape(len(yTest_le), 1)

enc = OneHotEncoder(categorical_features = [0])
yTrain_ohe = enc.fit_transform(yTrain_le).toarray()
yTest_ohe = enc.fit_transform(yTest_le).toarray()
print(yTest_ohe[400000])

"""##Neural Networks

###CNN
"""

#np.random.seed(7)

from keras.models import Sequential
from keras.layers import Conv1D, Conv2D, Flatten, Dense, Reshape
from keras import optimizers

model = Sequential()
model.add(Reshape((1,2,128), input_shape=(2,128)))
model.add(Conv2D(filters=64, kernel_size=(1,3), data_format='channels_first', activation='relu', padding='valid'))
model.add(Conv2D(filters=16, kernel_size=(2,3), data_format='channels_first', activation='relu', padding='valid'))
#zero padding to get 128?!
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint
	
es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=30)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)
model.fit(xTrain, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1000, epochs=100, verbose=1, callbacks=[es, mc])

#model.fit(xTrain, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1024, epochs=50, verbose=1)

"""###Evaluate Test set"""

from keras.models import load_model
saved_model = load_model('best_model.h5')

SNR = np.unique(labels[:,1])
SNR = list(map(int, SNR))
SNR.sort()
SNR = list(map(str, SNR))

accuracy = []

for s in SNR:
  y = []
  x = []
  M = np.where(yTest_SNR == s)
  for m in M[0]:
    y.append(yTest_ohe[m])
    x.append(xTest[m])
    
  y = np.array(y)
  x = np.array(x)
  loss, acc = saved_model.evaluate(x, y)
  print('Accuracy for SNR = ', s, ' is ', acc)
  accuracy.append(acc)
  
print(accuracy)

"""##Accuracy Plot"""

import matplotlib.pyplot as plt
plt.ylim(0, 1.0)
plt.plot(SNR, accuracy)

"""##Extra features

###Features Extraction - Integration
"""

import scipy

xTrainInteg = []
for x in xTrain:
  xInteg = np.array(scipy.integrate.simps(x))
  xTrainInteg.append(xInteg)
xTrainInteg = np.array(xTrainInteg)
print(xTrainInteg.shape)
print(xTrainInteg)

xTestInteg = []
for x in xTest:
  xInteg = np.array(scipy.integrate.simps(x))
  xTestInteg.append(xInteg)
xTestInteg = np.array(xTestInteg)

"""####CNN"""

#np.random.seed(7)

from keras.models import Sequential
from keras.layers import Conv1D, Conv2D, Flatten, Dense, Reshape
from keras import optimizers

model2 = Sequential()
model2.add(Reshape((1,1,2), input_shape=(1,2)))
model2.add(Conv2D(filters=64, kernel_size=(1,2), data_format='channels_last', activation='relu', padding='valid'))
model2.add(Conv2D(filters=16, kernel_size=(1,1), data_format='channels_last', activation='relu', padding='valid'))
#zero padding to get 128?!
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(10, activation='softmax'))

model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model2.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint
	
es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=30)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)
model.fit(xTrain, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1000, epochs=100, verbose=1, callbacks=[es, mc])

#model.fit(xTrain, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1024, epochs=50, verbose=1)

from keras.models import load_model
saved_model = load_model('best_model.h5')

SNR = np.unique(labels[:,1])
SNR = list(map(int, SNR))
SNR.sort()
SNR = list(map(str, SNR))

accuracy = []

for s in SNR:
  y = []
  x = []
  M = np.where(yTest_SNR == s)
  for m in M[0]:
    y.append(yTest_ohe[m])
    x.append(xTest[m])
    
  y = np.array(y)
  x = np.array(x)
  loss, acc = saved_model.evaluate(x, y)
  print('Accuracy for SNR = ', s, ' is ', acc)
  accuracy.append(acc)
  
print(accuracy)

"""###Features Extraction - Differentiation"""

xTrainDiff = []
for x in xTrain:
  xDiff = np.array(np.gradient(x))
  xTrainDiff.append(np.vstack((xDiff[0], xDiff[1])))
xTrainDiff = np.array(xTrainDiff)
print(xTrainDiff.shape)
#print(xTrainDiff)

xTestDiff = []
for x in xTest:
  xDiff = np.array(np.gradient(x))
  xTestDiff.append(np.vstack((xDiff[0], xDiff[1])))
xTestDiff = np.array(xTestDiff)

#np.random.seed(7)

from keras.models import Sequential
from keras.layers import Conv1D, Conv2D, Flatten, Dense, Reshape
from keras import optimizers

model2 = Sequential()
model2.add(Reshape((1,4,128), input_shape=(4,128)))
model2.add(Conv2D(filters=64, kernel_size=(1,3), data_format='channels_first', activation='relu', padding='valid'))
model2.add(Conv2D(filters=16, kernel_size=(2,3), data_format='channels_first', activation='relu', padding='valid'))
#zero padding to get 128?!
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(10, activation='softmax'))

model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model2.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint
	
es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=30)
mc = ModelCheckpoint('best_model2.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)
model2.fit(xTrainDiff, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1000, epochs=100, verbose=1, callbacks=[es, mc])

#model.fit(xTrain, yTrain_ohe, validation_split=0.05, shuffle=True, batch_size=1024, epochs=50, verbose=1)

from keras.models import load_model
saved_model2 = load_model('best_model2.h5')

SNR = np.unique(labels[:,1])
SNR = list(map(int, SNR))
SNR.sort()
SNR = list(map(str, SNR))

accuracy = []

for s in SNR:
  y = []
  x = []
  M = np.where(yTest_SNR == s)
  for m in M[0]:
    y.append(yTest_ohe[m])
    x.append(xTestDiff[m])
    
  y = np.array(y)
  x = np.array(x)
  loss, acc = saved_model2.evaluate(x, y)
  print('Accuracy for SNR = ', s, ' is ', acc)
  accuracy.append(acc)
  
print(accuracy)

import matplotlib.pyplot as plt
plt.ylim(0, 1.0)
plt.plot(SNR, accuracy)